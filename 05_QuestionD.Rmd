---
title: "Statistics for Data Science Project 2021/2022 - Risk of Business Failure"
subtitle: "Question D"
output: 
  html_notebook:
    toc: true
    theme:
      bootswatch: cerulean 
---
By:  
Carla Lorena Trejo Silva\
Jordanos Feyissa Gemechu\
Bruno Javier Limon Avila\

# Data Preparation
***
```{r}
library(caret)
library(ROCR)
library(car)
library(corrplot)
library(stringr)
library(dplyr)
```
```{r}
# cleaning the environment
rm(list=ls())
# loading the dataset
load(file="aida.RData")
# loading functions to be used
source("functions.R")
```
```{r}
# features added by using the functions in "functions.R"
aida <- getSize(aida)
aida <- getGeoArea(aida)
aida <- getStatus(aida)
aida <- getAge(aida)
# separating dataset filtering by year
aida.currYr <- getCurrYr(aida)
aida.yr1 <- getYr1(aida)
aida.yr2 <- getYr2(aida)
```
# 5.0 Question D
***
Fit a parametric model, and use it for failure scoring:

## 5.1 First Approach Logistic Regression Model
***
### 5.1.1 Preprocessing
```{r}
year_filter <- c("2014", "2015", "2016", "2017")
logreg_data <- filter(aida, `Last accounting closing date` %in% year_filter)
logreg_data <- na.omit(logreg_data)

# droping unnecessary columns for the logistic regression model
droplist <- c('ATECO 2007code','Company name','File','Tax code number','Legal form','Legal status','Province','Registered office address - Commune ISTAT code','Registered office address - Region','Incorporation year','Last accounting closing date', 'ATECO.Sector.Description', 'ATECO.Sector.Name')
logreg_data <- logreg_data[, !colnames(logreg_data) %in% droplist]

# turning the class 'Status' into 1 for active and 0 for failed
logreg_data$Status <- ifelse(logreg_data$Status == "Failed", 1, 0)
logreg_data$Status <- as.factor(logreg_data$Status)
# turning the variable 'geographical area' into 0 for south, 1 for center and 2 for north
logreg_data$GeoArea <- unclass(logreg_data$GeoArea)
logreg_data$GeoArea <- as.factor(logreg_data$GeoArea)

# 0 = Failed, 1 = Active
table(logreg_data$Status)

rm(droplist)
```

```{r}
# Split train and test data
# Holdout method; 70% training data
set.seed(42)
holdout <- createDataPartition(logreg_data$Status, p=0.7, list = F) 
X_train <- logreg_data[holdout, ]
X_test <- logreg_data[-holdout, ]

table(X_train$Status)

rm(holdout)
```

```{r}
logreg <- glm(Status ~ ., family = "binomial", data=X_train)
```

```{r}
pred <- predict(logreg, newdata = X_test, type = "response")
head(pred)
```
```{r}
y_pred_num <- ifelse(pred > .5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
confusionMatrix(y_pred, X_test$Status, positive="0", mode = "prec_recall")
```
```{r}
ROCpredict <- prediction(pred, X_test$Status)
ROCperformance <- performance(ROCpredict, "tpr", "fpr")
AUCscore = performance(ROCpredict, "auc")
AUClegend <- sprintf(AUCscore@y.values[[1]], fmt = '%#.4f') 

plot(ROCperformance, color="#a8a8a8", main="ROC Curve performance for first approach")
abline(0, 1, lwd=0.5, lty=2)
legend("bottomright", legend=sprintf("AUC = %s",AUClegend))
```
```{r}
calibrationData = calibration(X_test$Status ~ pred, class="1")

plot(calibrationData$data$midpoint, calibrationData$data$Percent, main="Calibration plot for first approach", col="blue", type="o", xlab="Prediction", ylab="Observation", ylim=c(0,100))
abline(0, 1, lwd=0.5, lty=2)
legend("bottomright", legend=c("Testing set", "Perfect calibration"), col=c("blue", "black"), pch=c("o","*",""),lty=c(2,2))
```
## 5.2 Improving Logistic Regression Model
***
### 5.2.1 Multicolinearity
***

```{r}
aida.cor <- filter(aida, `Last accounting closing date` %in% year_filter)
aida.cor <- na.omit(aida)

# droping unnecessary columns for the logistic regression model
droplist <- c('ATECO 2007code','Company name','File','Tax code number','Legal form','Legal status','Province','Registered office address - Commune ISTAT code','Registered office address - Region','Incorporation year','Last accounting closing date', 'ATECO.Sector.Description', 'ATECO.Sector.Name')
aida.cor <- aida.cor[, !colnames(aida.cor) %in% droplist]

corMatrix <- cor(aida.cor[,0:70])
to.remove = findCorrelation(corMatrix, cutoff = .70, verbose = TRUE, names=TRUE)
to.remove 
```

```{r}
logreg_data_cor1 <- filter(aida.currYr, `Last accounting closing date` %in% year_filter)
logreg_data_cor1 <- na.omit(logreg_data_cor1)

# droping unnecessary columns for the logistic regression model
droplist <- c('ATECO 2007code','Company name','File','Tax code number','Legal form','Legal status','Province','Registered office address - Commune ISTAT code','Registered office address - Region','Incorporation year','Last accounting closing date', 'ATECO.Sector.Description', 'ATECO.Sector.Name')
logreg_data_cor1 <- logreg_data_cor1[, !colnames(logreg_data_cor1) %in% droplist]

# turning the class 'Status' into 1 for active and 0 for failed
logreg_data_cor1$Status <- ifelse(logreg_data_cor1$Status == "Failed", 1, 0)
logreg_data_cor1$Status <- as.factor(logreg_data_cor1$Status)
# turning the variable 'geographical area' into 0 for south, 1 for center and 2 for north
logreg_data_cor1$GeoArea <- unclass(logreg_data_cor1$GeoArea)
logreg_data_cor1$GeoArea <- as.factor(logreg_data_cor1$GeoArea)

# Split train and test data
# Holdout method; 70% training data
set.seed(42)
holdout <- createDataPartition(logreg_data_cor1$Status, p=0.7, list = F) 
X_train_cor1 <- logreg_data_cor1[holdout, ]
X_test_cor1 <- logreg_data_cor1[-holdout, ]

logreg_cor1 <- glm(Status ~ ., family = "binomial", data=X_train_cor1)
summary(logreg_cor1)

rm(holdout)
```
```{r}
pred_cor1 <- predict(logreg_cor1, newdata = X_test_cor1, type = "response")
y_pred_num_cor1 <- ifelse(pred_cor1 > 0.5, 1, 0)
y_pred_cor1 <- factor(y_pred_num_cor1, levels=c(0, 1))

confusionMatrix(y_pred_cor1, X_test_cor1$Status, positive="0", mode = "prec_recall")
```

```{r}
corMatrix <- cor(logreg_data_cor1[,0:24])
highcorrelation = findCorrelation(corMatrix, cutoff = .70, verbose = TRUE, names=TRUE)
highcorrelation 
```

```{r}
vif_values <- vif(logreg_cor1) 
barplot(vif_values[,1], main = "VIF Values", ylab = "Features", xlab="Variance Inflation Factor", horiz = TRUE, col = "steelblue")
abline(v = 5, lwd = 3, lty = 2)
```

```{r}
ROCpredict_cor1 <- prediction(pred_cor1, X_test_cor1$Status)
ROCperformance_cor1 <- performance(ROCpredict_cor1, "tpr", "fpr")
AUCscore_cor1 = performance(ROCpredict_cor1, "auc")
AUClegend_cor1 <- sprintf(AUCscore_cor1@y.values[[1]], fmt = '%#.4f') 

plot(ROCperformance_cor1, color="#a8a8a8", main="ROC Curve performance of Selected Features 1")
abline(0, 1, lwd=0.5, lty=2)
legend("bottomright", legend=sprintf("AUC = %s",AUClegend_cor1))
```

```{r}
calibrationData_cor1 = calibration(X_test_cor1$Status ~ pred_cor1, class="1")

plot(calibrationData_cor1$data$midpoint, calibrationData_cor1$data$Percent, main="Calibration plot for Selected Features 1", col="red", type="o", xlab="Prediction", ylab="Observation", ylim=c(0,100))
abline(0, 1, lwd=0.5, lty=2)
legend("bottomright", legend=c("Selected Features 1", "Perfect calibration"), col=c("red", "black"), pch=c("o","*"),lty=c(2,2))
```
```{r}
logreg_data_cor2 <- filter(aida.currYr, `Last accounting closing date` %in% year_filter)
logreg_data_cor2 <- na.omit(logreg_data_cor2)

# droping unnecessary columns for the logistic regression model
droplist <- c('ATECO 2007code','Company name','File','Tax code number','Legal form','Legal status','Province','Registered office address - Commune ISTAT code','Registered office address - Region','Incorporation year','Last accounting closing date', 'ATECO.Sector.Description', 'ATECO.Sector.Name', 'Total assetsth EURLast avail. yr', 'Return on asset (ROA)%Last avail. yr', 'EBITDAth EURLast avail. yr', 'Cash Flowth EURLast avail. yr', "Number of employeesLast avail. yr", "Return on sales (ROS)%Last avail. yr", "Liquidity ratioLast avail. yr", "Net working capitalth EURLast avail. yr", "Net financial positionth EURLast avail. yr")
logreg_data_cor2 <- logreg_data_cor2[, !colnames(logreg_data_cor2) %in% droplist]

# turning the class 'Status' into 1 for active and 0 for failed
logreg_data_cor2$Status <- ifelse(logreg_data_cor2$Status == "Failed", 1, 0)
logreg_data_cor2$Status <- as.factor(logreg_data_cor2$Status)
# turning the variable 'geographical area' into 0 for south, 1 for center and 2 for north
logreg_data_cor2$GeoArea <- unclass(logreg_data_cor2$GeoArea)
logreg_data_cor2$GeoArea <- as.factor(logreg_data_cor2$GeoArea)

# Split train and test data
# Holdout method; 70% training data
set.seed(42)
holdout <- createDataPartition(logreg_data_cor2$Status, p=0.7, list = F) 
X_train_cor2 <- logreg_data_cor2[holdout, ]
X_test_cor2 <- logreg_data_cor2[-holdout, ]

logreg_cor2 <- glm(Status ~ ., family = "binomial", data=X_train_cor2)
summary(logreg_cor2)

rm(holdout)
```

```{r}
pred_cor2 <- predict(logreg_cor2, newdata = X_test_cor2, type = "response")
y_pred_num_cor2 <- ifelse(pred_cor2 > 0.5, 1, 0)
y_pred_cor2 <- factor(y_pred_num_cor2, levels=c(0, 1))

confusionMatrix(y_pred_cor2, X_test_cor2$Status, positive="0", mode = "prec_recall")

```

```{r}
ROCpredict_cor2 <- prediction(pred_cor2, X_test_cor2$Status)
ROCperformance_cor2 <- performance(ROCpredict_cor2, "tpr", "fpr")
AUCscore_cor2 = performance(ROCpredict_cor2, "auc")
AUClegend_cor2 <- sprintf(AUCscore_cor2@y.values[[1]], fmt = '%#.4f') 

plot(ROCperformance_cor2, color="#a8a8a8", main="ROC Curve performance of Selected Features 2")
abline(0, 1, lwd=0.5, lty=2)
legend("bottomright", legend=sprintf("AUC = %s",AUClegend_cor2))
```

```{r}
calibrationData_cor2 = calibration(X_test_cor2$Status ~ pred_cor2, class="1")

plot(calibrationData_cor2$data$midpoint, calibrationData_cor2$data$Percent, main="Calibration plot for Selected Features 2", col="green", type="o", xlab="Prediction", ylab="Observation", ylim=c(0,100))
abline(0, 1, lwd=0.5, lty=2)
legend("bottomright", legend=c("Selected Features 2", "Perfect calibration"), col=c("green", "black"), pch=c("o","*"),lty=c(2,2))
```
### 5.2.2 Imbalanced target class
***
```{r}
logreg_data_imb <- filter(aida, `Last accounting closing date` %in% year_filter)
logreg_data_imb <- na.omit(logreg_data_imb)

# droping unnecessary columns for the logistic regression model
droplist <- c('ATECO 2007code','Company name','File','Tax code number','Legal form','Legal status','Province','Registered office address - Commune ISTAT code','Registered office address - Region','Incorporation year','Last accounting closing date', 'ATECO.Sector.Description', 'ATECO.Sector.Name')
logreg_data_imb <- logreg_data_imb[, !colnames(logreg_data_imb) %in% droplist]

# turning the class 'Status' into 1 for active and 0 for failed
logreg_data_imb$Status <- ifelse(logreg_data_imb$Status == "Failed", 1, 0)
logreg_data_imb$Status <- as.factor(logreg_data_imb$Status)
# turning the variable 'geographical area' into 0 for south, 1 for center and 2 for north
logreg_data_imb$GeoArea <- unclass(logreg_data_imb$GeoArea)
logreg_data_imb$GeoArea <- as.factor(logreg_data_imb$GeoArea)

# 0 = Failed, 1 = Active
table(logreg_data_imb$Status)

rm(droplist)
```
```{r}
# Split train and test data
# Holdout method; 70% training data
set.seed(42)
holdout <- createDataPartition(logreg_data_imb$Status, p=0.7, list = F) 
X_train_imb <- logreg_data_imb[holdout, ]
X_test_imb <- logreg_data_imb[-holdout, ]

# Rebalancing the frequency of failed companies
'%ni%' <- Negate('%in%')
set.seed(42)
X_train_oversample <- upSample(x = X_train_imb[, colnames(X_train_imb) %ni% "Status"], y = X_train_imb$Status)
table(X_train_oversample$Class)

rm(holdout)
```

```{r}
logreg_imb <- glm(Class ~ ., family = "binomial", data=X_train_oversample)
#summary(logreg_imb)

pred_imb <- predict(logreg_imb, newdata = X_test_imb, type = "response")
y_pred_num_imb <- ifelse(pred_imb > 0.5, 1, 0)
y_pred_imb <- factor(y_pred_num_imb, levels=c(0, 1))

confusionMatrix(y_pred_imb, X_test_imb$Status, positive="0", mode = "prec_recall")
```

```{r}
ROCpredict_imb <- prediction(pred_imb, X_test_imb$Status)
ROCperformance_imb <- performance(ROCpredict_imb, "tpr", "fpr")
AUCscore_imb = performance(ROCpredict_imb, "auc")
AUClegend_imb <- sprintf(AUCscore_imb@y.values[[1]], fmt = '%#.4f') 

plot(ROCperformance_imb, color="#a8a8a8", main="ROC Curve performance of Rebalanced data")
abline(0, 1, lwd=0.5, lty=2)
legend("bottomright", legend=sprintf("AUC = %s",AUClegend_imb))
```
```{r}
calibrationData_imb = calibration(X_test_imb$Status ~ pred_imb, class="1")

plot(calibrationData_imb$data$midpoint, calibrationData_imb$data$Percent, main="Calibration plot for Rebalanced data", col="purple", type="o", xlab="Prediction", ylab="Observation", ylim=c(0,100))
abline(0, 1, lwd=0.5, lty=2)
legend("bottomright", legend=c("Testing set", "Perfect calibration"), col=c("purple", "black"), pch=c("o","*",""),lty=c(2,2))
```

### 5.2.3 Combining Methods
***
```{r}
logreg_data_comb <- filter(aida.currYr, `Last accounting closing date` %in% year_filter)
logreg_data_comb <- na.omit(logreg_data_comb)

# droping unnecessary columns for the logistic regression model
droplist <- c('ATECO 2007code','Company name','File','Tax code number','Legal form','Legal status','Province','Registered office address - Commune ISTAT code','Registered office address - Region','Incorporation year','Last accounting closing date', 'ATECO.Sector.Description', 'ATECO.Sector.Name')
logreg_data_comb <- logreg_data_comb[, !colnames(logreg_data_comb) %in% droplist]

# turning the class 'Status' into 1 for active and 0 for failed
logreg_data_comb$Status <- ifelse(logreg_data_comb$Status == "Failed", 1, 0)
logreg_data_comb$Status <- as.factor(logreg_data_comb$Status)
# turning the variable 'geographical area' into 0 for south, 1 for center and 2 for north
logreg_data_comb$GeoArea <- unclass(logreg_data_comb$GeoArea)
logreg_data_comb$GeoArea <- as.factor(logreg_data_comb$GeoArea)

# 0 = Failed, 1 = Active
table(logreg_data_comb$Status)

rm(droplist)
```
```{r}
# Split train and test data
# Holdout method; 70% training data
set.seed(42)
holdout <- createDataPartition(logreg_data_comb$Status, p=0.7, list = F) 
X_train_comb <- logreg_data_comb[holdout, ]
X_test_comb <- logreg_data_comb[-holdout, ]

# Rebalancing the frequency of failed companies
'%ni%' <- Negate('%in%')
set.seed(42)
X_train_oversample_comb <- upSample(x = X_train_comb[, colnames(X_train_comb) %ni% "Status"], y = X_train_comb$Status)
table(X_train_oversample_comb$Class)

rm(holdout)
```

```{r}
logreg_comb <- glm(Class ~ ., family = "binomial", data=X_train_oversample_comb)
#summary(logreg_comb)

pred_comb <- predict(logreg_comb, newdata = X_test_comb, type = "response")
y_pred_num_comb <- ifelse(pred_comb > 0.5, 1, 0)
y_pred_comb <- factor(y_pred_num_comb, levels=c(0, 1))

confusionMatrix(y_pred_comb, X_test_comb$Status, positive="0", mode = "prec_recall")
```

```{r}
ROCpredict_comb <- prediction(pred_comb, X_test_comb$Status)
ROCperformance_comb <- performance(ROCpredict_comb, "tpr", "fpr")
AUCscore_comb = performance(ROCpredict_comb, "auc")
AUClegend_comb <- sprintf(AUCscore_comb@y.values[[1]], fmt = '%#.4f') 

plot(ROCperformance_comb, color="#a8a8a8", main="ROC Curve performance of Combining methods")
abline(0, 1, lwd=0.5, lty=2)
legend("bottomright", legend=sprintf("AUC = %s",AUClegend_comb))
```
```{r}
calibrationData_comb = calibration(X_test_comb$Status ~ pred_comb, class="1")

plot(calibrationData_comb$data$midpoint, calibrationData_comb$data$Percent, main="Calibration plot for Combining methods", col="darkorange", type="o", xlab="Prediction", ylab="Observation", ylim=c(0,100))
abline(0, 1, lwd=0.5, lty=2)
legend("bottomright", legend=c("Testing set", "Perfect calibration"), col=c("darkorange", "black"), pch=c("o","*",""),lty=c(2,2))
```
## 5.3 Model evaluation
***
```{r}
plot(ROCperformance, col="blue", main="ROC Curve performance comparison between different approaches")
par(new=TRUE)
plot(ROCperformance_cor1, col="red")
par(new=TRUE)
plot(ROCperformance_cor2, col="green")
par(new=TRUE)
plot(ROCperformance_imb, col="purple")
par(new=TRUE)
plot(ROCperformance_comb, col="darkorange")
abline(0, 1, lwd=0.5, lty=2)

legend("bottomright", legend=c(sprintf("BaseModel AUC = %s",AUClegend),sprintf("SelectedFeatures1 AUC = %s",AUClegend_cor1),sprintf("SelectedFeatures2 AUC = %s",AUClegend_cor2),sprintf("RebalancedData AUC = %s",AUClegend_imb),sprintf("CombiningMethods AUC = %s",AUClegend_comb)), col=c("blue","red","green","purple","darkorange"), pch="_")
```

```{r}
plot(calibrationData$data$midpoint, calibrationData$data$Percent, main="Calibration plot comparison between different approaches", col="blue", type="o", xlab="Predicted probability", ylab="Observed probability", ylim=c(0,100))
lines(calibrationData_cor1$data$midpoint, calibrationData_cor1$data$Percent, col="red", type="o")
lines(calibrationData_cor2$data$midpoint, calibrationData_cor2$data$Percent, col="green", type="o")
lines(calibrationData_imb$data$midpoint, calibrationData_imb$data$Percent, col="purple", type="o")
lines(calibrationData_comb$data$midpoint, calibrationData_comb$data$Percent, col="darkorange", type="o")
abline(0, 1, lwd=0.5, lty=2)
legend("bottomright", legend=c("Base model","SelectedFeatures1","SelectedFeatures2","Rebalanced data","CombiningMethods"), col=c("blue","red","green","purple","darkorange"), pch=c("o","o","o","o","o"),lty=2)
```
```{r}
plot(density(logreg$fitted.values), ylim=c(0,3), main="Density function of fitted values for different methods", col="blue", xlab="Log model fitted values")
lines(density(logreg_cor1$fitted.values), col="red")
lines(density(logreg_cor2$fitted.values), col="green")
lines(density(logreg_imb$fitted.values), col="purple")
lines(density(logreg_comb$fitted.values), col="darkorange")

legend("topright", legend=c("Base model","SelectedFeatures1","SelectedFeatures2","Rebalanced data","CombiningMethods"), col=c("blue","red","green","purple","darkorange"), pch="_")
```
```{r}
var.test(logreg$fitted.values, logreg_cor1$fitted.values)
var.test(logreg$fitted.values, logreg_cor2$fitted.values)
var.test(logreg$fitted.values, logreg_imb$fitted.values)
var.test(logreg$fitted.values, logreg_comb$fitted.values)
```
```{r}
t.test(logreg$fitted.values, logreg_cor1$fitted.values)
t.test(logreg$fitted.values, logreg_cor2$fitted.values)
t.test(logreg$fitted.values, logreg_imb$fitted.values)
t.test(logreg$fitted.values, logreg_comb$fitted.values)
```
```{r}
wilcox.test(logreg$fitted.values, logreg_cor1$fitted.values)
wilcox.test(logreg$fitted.values, logreg_cor2$fitted.values)
wilcox.test(logreg$fitted.values, logreg_imb$fitted.values)
wilcox.test(logreg$fitted.values, logreg_comb$fitted.values)
```
```{r}
plot(density(logreg$residuals), ylim=c(0,.6), xlim=c(-10,10), main="Density function of residuals for different methods", col="blue", xlab="Log model residuals")
lines(density(logreg_cor1$residuals), col="red")
lines(density(logreg_cor2$residuals), col="green")
lines(density(logreg_imb$residuals), col="purple")
lines(density(logreg_comb$residuals), col="darkorange")

legend("topright", legend=c("Base model","SelectedFeatures1","SelectedFeatures2","Rebalanced data","CombiningMethods"), col=c("blue","red","green","purple","darkorange"), pch="_")
```
```{r}
var.test(logreg$residuals, logreg_cor1$residuals)
var.test(logreg$residuals, logreg_cor2$residuals)
var.test(logreg$residuals, logreg_imb$residuals)
var.test(logreg$residuals, logreg_comb$residuals)
```
```{r}
t.test(logreg$residuals, logreg_cor1$residuals)
t.test(logreg$residuals, logreg_cor2$residuals)
t.test(logreg$residuals, logreg_imb$residuals)
t.test(logreg$residuals, logreg_comb$residuals)
```
```{r}
wilcox.test(logreg$residuals, logreg_cor1$residuals)
wilcox.test(logreg$residuals, logreg_cor2$residuals)
wilcox.test(logreg$residuals, logreg_imb$residuals)
wilcox.test(logreg$residuals, logreg_comb$residuals)
```

***
## 5.4 Rating model
```{r}
year_filter <- c("2014", "2015", "2016", "2017")
logreg_data_rating <- filter(aida, `Last accounting closing date` %in% year_filter)
logreg_data_rating <- na.omit(logreg_data_rating)

# droping unnecessary columns for the logistic regression model
droplist <- c('ATECO 2007code','Company name','File','Tax code number','Legal form','Legal status','Province','Registered office address - Commune ISTAT code','Registered office address - Region','Incorporation year','Last accounting closing date', 'ATECO.Sector.Description', 'ATECO.Sector.Name')
logreg_data_rating <- logreg_data_rating[, !colnames(logreg_data_rating) %in% droplist]

# turning the class 'Status' into 1 for active and 0 for failed
logreg_data_rating$Status <- ifelse(logreg_data_rating$Status == "Failed", 1, 0)
logreg_data_rating$Status <- as.factor(logreg_data_rating$Status)
# turning the variable 'geographical area' into 0 for south, 1 for center and 2 for north
logreg_data_rating$GeoArea <- unclass(logreg_data_rating$GeoArea)
logreg_data_rating$GeoArea <- as.factor(logreg_data_rating$GeoArea)

# Split train and test data
# Holdout method; 70% training data
set.seed(42)
holdout <- createDataPartition(logreg_data_rating$Status, p=0.7, list = F) 
X_train_rating <- logreg_data_rating[holdout, ]
X_test_rating <- logreg_data_rating[-holdout, ]

rm(droplist, holdout)

logreg_rating <- glm(Status ~ ., family = "binomial", data=X_train_rating)
#summary(logreg)

pred_rating <- predict(logreg_rating, newdata = X_test_rating, type = "response")
head(pred_rating)
```

```{r}
y_pred_num_rating1 <- ifelse(pred_rating > .1, 1, 0)
sum(y_pred_num_rating1 == 1)
sum(y_pred_num_rating1 == 0)

y_pred_num_rating2 <- ifelse(pred_rating > .2, 1, 0)
sum(y_pred_num_rating2 == 1)
sum(y_pred_num_rating2 == 0)

y_pred_num_rating3 <- ifelse(pred_rating > .3, 1, 0)
sum(y_pred_num_rating3 == 1)
sum(y_pred_num_rating3 == 0)

y_pred_num_rating4 <- ifelse(pred_rating > .4, 1, 0)
sum(y_pred_num_rating4 == 1)
sum(y_pred_num_rating4 == 0)

y_pred_num_rating5 <- ifelse(pred_rating > .5, 1, 0)
sum(y_pred_num_rating5 == 1)
sum(y_pred_num_rating5 == 0)
```
